# -*- coding: utf-8 -*-
"""CNN_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QzzqLQbaifsgIZm9KnKpir8Hf7W0WvuV
"""

pip install torch torchvision

import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.nn as nn
from torch.optim.lr_scheduler import StepLR
import torch.optim as optim


# Transformations applied on each image
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize the images to 224x224 for pretrained models
    transforms.ToTensor(),  # Convert images to PyTorch tensors
    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Repeat grayscale channel 3 times to match RGB input
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize using ImageNet values
])

# Downloading the MNIST dataset
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

# DataLoader
train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

# Transformations applied on each image of FMNIST dataset
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize the images to 224x224 for pretrained models
    transforms.ToTensor(),  # Convert images to PyTorch tensors
    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Repeat grayscale channel 3 times to match RGB input
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize using ImageNet values
])

# Downloading the Fashion MNIST dataset
fmnist_train = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)
fmnist_test = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)

# DataLoader
fmnist_train_loader = DataLoader(fmnist_train, batch_size=32, shuffle=True)
fmnist_test_loader = DataLoader(fmnist_test, batch_size=32, shuffle=False)

# Transformations applied on each image of CIFAR-10 dataset
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize the images to 224x224 for pretrained models
    transforms.ToTensor(),  # Convert images to PyTorch tensors
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Mean and std for CIFAR-10
])
# Downloading the CIFAR-10 dataset
Cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
Cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# DataLoader
Cifar_train_loader = DataLoader(Cifar_train, batch_size=32, shuffle=True, num_workers=2)
Cifar_test_loader = DataLoader(Cifar_test, batch_size=32, shuffle=False, num_workers=2)

import numpy as np

# Define the LeNet-5 architecture
class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)
        # Adaptive layer to handle varying input sizes
        self.adaptive_pool = nn.AdaptiveAvgPool2d((5, 5))
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        # Adaptive pooling to ensure consistent output size
        x = self.adaptive_pool(x)
        x = x.view(-1, 16 * 5 * 5)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Initialize the network and move to GPU if available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net = LeNet5().to(device)

# Define optimizer, loss function, and learning rate scheduler
optimizer = optim.Adam(net.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# Initialize weights
def initialize_weights(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
        nn.init.kaiming_uniform_(m.weight)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)

net.apply(initialize_weights)

# Training and Evaluation function for MNIST dataset
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Training function with loss plot for MNIST
def train(epochs, net, train_loader, optimizer, scheduler):
    net.train()
    loss_history = []

    for epoch in range(epochs):
        epoch_loss = 0
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = net(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        # Average loss for the epoch
        avg_loss = epoch_loss / len(train_loader)
        loss_history.append(avg_loss)
        scheduler.step()
        print(f"Epoch {epoch+1}/{epochs} completed. Loss: {avg_loss:.4f}")

    # Plot the training loss over epochs
    plt.plot(range(1, epochs + 1), loss_history, marker='o')
    plt.title("Training Loss over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Average Loss")
    plt.show()

# Evaluation function with performance metrics and ROC curves
def evaluate(net, test_loader):
    net.eval()
    all_targets = []
    all_preds = []
    all_outputs = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = net(data)
            pred = output.argmax(dim=1, keepdim=True).view(-1)
            all_targets.extend(target.cpu().numpy())
            all_preds.extend(pred.cpu().numpy())
            all_outputs.extend(output.cpu().numpy())

    # Calculate overall metrics
    accuracy = accuracy_score(all_targets, all_preds)
    precision = precision_score(all_targets, all_preds, average='macro')
    recall = recall_score(all_targets, all_preds, average='macro')
    f1 = f1_score(all_targets, all_preds, average='macro')

    print("Classification Report:")
    print(classification_report(all_targets, all_preds))
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Binarize labels for ROC curve calculation
    num_classes = len(np.unique(all_targets))
    targets_one_hot = label_binarize(all_targets, classes=np.arange(num_classes))
    all_outputs = np.array(all_outputs)

    # Plot ROC curves for each class
    plt.figure(figsize=(10, 8))
    for i in range(num_classes):
        fpr, tpr, _ = roc_curve(targets_one_hot[:, i], all_outputs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Each Class')
    plt.legend(loc="lower right")
    plt.show()

# Loading LeNet-5, AlexNet, GoogleNet and ResNet for MNIST
LeNet = net
AlexNet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)
AlexNet = AlexNet.to(device)
GoogLeNet = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)
GoogLeNet = GoogLeNet.to(device)
ResNet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)
ResNet = ResNet.to(device)

# Modify output layers  (assuming num_classes = 10 here)
num_classes = 10
AlexNet.classifier[6] = torch.nn.Linear(AlexNet.classifier[6].in_features, num_classes)
GoogLeNet.fc = torch.nn.Linear(GoogLeNet.fc.in_features, num_classes)
ResNet.fc = torch.nn.Linear(ResNet.fc.in_features, num_classes)

# Define a list of networks and move to GPU if available
networks = [LeNet,AlexNet,GoogLeNet,ResNet]

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Iterate over each network
for net in networks:
 net = net.to(device)

 # Initialize criterion, optimizer, and scheduler for each model
 criterion = torch.nn.CrossEntropyLoss()
 optimizer = optim.Adam(net.parameters(), lr=0.001)
 scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)

 # Train and evaluate the models for the MNIST dataset
 print(f"Training {net}")
 train(5, net, train_loader, optimizer, scheduler)

 print(f"Evaluating {net}")
 evaluate(net, test_loader)

pip install timm

# Train and evaluate VGG, Xception and SENet for MNIST dataset
import timm
import numpy as np

# Load VGGNet
VGGNet = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)
VGGNet=VGGNet.to(device)

# Load Xception
Xception = timm.create_model('xception', pretrained=True)
Xception = Xception.to(device)

# Load SENet (e.g., SENet154)
SENet = timm.create_model('senet154', pretrained=True)
SENet = SENet.to(device)

# Modify output layers
num_classes = 10
VGGNet.classifier[6] = torch.nn.Linear(VGGNet.classifier[6].in_features, num_classes)
Xception.fc = torch.nn.Linear(Xception.fc.in_features, num_classes)
SENet.fc = torch.nn.Linear(SENet.fc.in_features, num_classes)

# Initialize the network and move to GPU if available
networks = [VGGNet,Xception,SENet]

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Iterate over each network
for net in networks:
 net = net.to(device)

 # Initialize criterion, optimizer, and scheduler for each model
 criterion = torch.nn.CrossEntropyLoss()
 optimizer = optim.Adam(net.parameters(), lr=0.001)
 scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)

 # Train and evaluate the remaining models for the MNIST dataset
 print(f"Training {net}")
 train(5, net, train_loader, optimizer, scheduler)

 print(f"Evaluating {net}")
 evaluate(net, test_loader)

"""Now repeating the training and evaluation process for **FMNIST** dataset for each architecture."""

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Training function with loss plot for FMNIST dataset
def train(epochs, net, fmnist_train_loader, optimizer, scheduler):
    net.train()
    loss_history = []

    for epoch in range(epochs):
        epoch_loss = 0
        for data, target in fmnist_train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = net(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        # Average loss for the epoch
        avg_loss = epoch_loss / len(fmnist_train_loader)
        loss_history.append(avg_loss)
        scheduler.step()
        print(f"Epoch {epoch+1}/{epochs} completed. Loss: {avg_loss:.4f}")

    # Plot the training loss over epochs
    plt.plot(range(1, epochs + 1), loss_history, marker='o')
    plt.title("Training Loss over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Average Loss")
    plt.show()

# Evaluation function with performance metrics and ROC curves for FMNIST
def evaluate(net, fmnist_test_loader):
    net.eval()
    all_targets = []
    all_preds = []
    all_outputs = []

    with torch.no_grad():
        for data, target in fmnist_test_loader:
            data, target = data.to(device), target.to(device)
            output = net(data)
            pred = output.argmax(dim=1, keepdim=True).view(-1)
            all_targets.extend(target.cpu().numpy())
            all_preds.extend(pred.cpu().numpy())
            all_outputs.extend(output.cpu().numpy())

    # Calculate overall metrics
    accuracy = accuracy_score(all_targets, all_preds)
    precision = precision_score(all_targets, all_preds, average='macro')
    recall = recall_score(all_targets, all_preds, average='macro')
    f1 = f1_score(all_targets, all_preds, average='macro')

    print("Classification Report:")
    print(classification_report(all_targets, all_preds))
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Binarize labels for ROC curve calculation
    num_classes = len(np.unique(all_targets))
    targets_one_hot = label_binarize(all_targets, classes=np.arange(num_classes))
    all_outputs = np.array(all_outputs)

    # Plot ROC curves for each class
    plt.figure(figsize=(10, 8))
    for i in range(num_classes):
        fpr, tpr, _ = roc_curve(targets_one_hot[:, i], all_outputs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Each Class')
    plt.legend(loc="lower right")
    plt.show()

# Loading LeNet-5, AlexNet, GoogleNet, ResNet, VggNet, Xception and SENet For FMNIST
LeNet = net
AlexNet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)
AlexNet = AlexNet.to(device)
GoogLeNet = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)
GoogLeNet = GoogLeNet.to(device)
ResNet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)
ResNet = ResNet.to(device)
VGGNet = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)
VGGNet=VGGNet.to(device)
Xception = timm.create_model('xception', pretrained=True)
Xception = Xception.to(device)
SENet = timm.create_model('senet154', pretrained=True)
SENet = SENet.to(device)

# Modify output layers  (assuming num_classes = 10 here)
num_classes = 10

AlexNet.classifier[6] = torch.nn.Linear(AlexNet.classifier[6].in_features, num_classes)
GoogLeNet.fc = torch.nn.Linear(GoogLeNet.fc.in_features, num_classes)
ResNet.fc = torch.nn.Linear(ResNet.fc.in_features, num_classes)
VGGNet.classifier[6] = torch.nn.Linear(VGGNet.classifier[6].in_features, num_classes)
Xception.fc = torch.nn.Linear(Xception.fc.in_features, num_classes)
SENet.fc = torch.nn.Linear(SENet.fc.in_features, num_classes)

# define a list of networks and move to GPU if available
networks = [LeNet,AlexNet,GoogLeNet,ResNet,VGGNet,Xception,SENet]

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Iterate over each network
for net in networks:
 net = net.to(device)

 # Initialize criterion, optimizer, and scheduler for each model
 criterion = torch.nn.CrossEntropyLoss()
 optimizer = optim.Adam(net.parameters(), lr=0.001)
 scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)

 # Train and evaluate the models for the FMNIST dataset
 print(f"Training {net}")
 train(5, net, fmnist_train_loader, optimizer, scheduler)

 print(f"Evaluating {net}")
 evaluate(net, fmnist_test_loader)

"""Now repeating the training and evaluation process for **CIFAR-10** dataset for each architecture."""

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Training function with loss plot for CIFAR-10 dataset
def train(epochs, net, Cifar_train_loader, optimizer, scheduler):
    net.train()
    loss_history = []

    for epoch in range(epochs):
        epoch_loss = 0
        for data, target in Cifar_train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = net(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        # Average loss for the epoch
        avg_loss = epoch_loss / len(Cifar_train_loader)
        loss_history.append(avg_loss)
        scheduler.step()
        print(f"Epoch {epoch+1}/{epochs} completed. Loss: {avg_loss:.4f}")

    # Plot the training loss over epochs
    plt.plot(range(1, epochs + 1), loss_history, marker='o')
    plt.title("Training Loss over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Average Loss")
    plt.show()

# Evaluation function with performance metrics and ROC curves for CIFAR-10
def evaluate(net, Cifar_test_loader):
    net.eval()
    all_targets = []
    all_preds = []
    all_outputs = []

    with torch.no_grad():
        for data, target in Cifar_test_loader:
            data, target = data.to(device), target.to(device)
            output = net(data)
            pred = output.argmax(dim=1, keepdim=True).view(-1)
            all_targets.extend(target.cpu().numpy())
            all_preds.extend(pred.cpu().numpy())
            all_outputs.extend(output.cpu().numpy())

    # Calculate overall metrics
    accuracy = accuracy_score(all_targets, all_preds)
    precision = precision_score(all_targets, all_preds, average='macro')
    recall = recall_score(all_targets, all_preds, average='macro')
    f1 = f1_score(all_targets, all_preds, average='macro')

    print("Classification Report:")
    print(classification_report(all_targets, all_preds))
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Binarize labels for ROC curve calculation
    num_classes = len(np.unique(all_targets))
    targets_one_hot = label_binarize(all_targets, classes=np.arange(num_classes))
    all_outputs = np.array(all_outputs)

    # Plot ROC curves for each class
    plt.figure(figsize=(10, 8))
    for i in range(num_classes):
        fpr, tpr, _ = roc_curve(targets_one_hot[:, i], all_outputs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Each Class')
    plt.legend(loc="lower right")
    plt.show()

import timm
import numpy as np

# Loading LeNet-5, AlexNet, GoogleNet, ResNet, VggNet, Xception and SENet For CIFAR-10
LeNet = net
AlexNet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)
AlexNet = AlexNet.to(device)
GoogLeNet = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)
GoogLeNet = GoogLeNet.to(device)
ResNet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)
ResNet = ResNet.to(device)
VGGNet = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)
VGGNet=VGGNet.to(device)
Xception = timm.create_model('xception', pretrained=True)
Xception = Xception.to(device)
SENet = timm.create_model('senet154', pretrained=True)
SENet = SENet.to(device)

# Modify output layers  (assuming num_classes = 10 here)
num_classes = 10

AlexNet.classifier[6] = torch.nn.Linear(AlexNet.classifier[6].in_features, num_classes)
GoogLeNet.fc = torch.nn.Linear(GoogLeNet.fc.in_features, num_classes)
ResNet.fc = torch.nn.Linear(ResNet.fc.in_features, num_classes)
VGGNet.classifier[6] = torch.nn.Linear(VGGNet.classifier[6].in_features, num_classes)
Xception.fc = torch.nn.Linear(Xception.fc.in_features, num_classes)
SENet.fc = torch.nn.Linear(SENet.fc.in_features, num_classes)

# define a list of networks and move to GPU if available
networks = [LeNet,AlexNet,GoogLeNet,ResNet,VGGNet,Xception,SENet]

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Iterate over each network
for net in networks:
 net = net.to(device)

 # Initialize criterion, optimizer, and scheduler for each model
 criterion = torch.nn.CrossEntropyLoss()
 # Optimizer with an adaptive learning rate based on batch size
 initial_lr = 0.001  # Base learning rate
 optimizer = optim.Adam(net.parameters(), lr=initial_lr )

 # Learning rate scheduler with warmup
 scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR every 5 epochs

 # Train and evaluate the models for the CIFAR-10 dataset
 print(f"Training {net}")
 train(10, net, Cifar_train_loader, optimizer, scheduler)

 print(f"Evaluating {net}")
 evaluate(net, Cifar_test_loader)