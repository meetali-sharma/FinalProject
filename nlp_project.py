# -*- coding: utf-8 -*-
"""NLP_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m7jqhn0BLcSFajqlO010Ww5xpXJmnm15
"""

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile NLP.py
# 
# import streamlit as st
# from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer, pipeline, AutoTokenizer, AutoModelForQuestionAnswering, BartForConditionalGeneration, BartTokenizer
# import torch
# from diffusers import StableDiffusionPipeline
# from PIL import Image
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# 
# # Load models and tokenizers
# gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')
# gpt2_model.eval()
# 
# chat_model_name = "microsoft/DialoGPT-medium"
# chat_tokenizer = AutoTokenizer.from_pretrained(chat_model_name, padding_side='left')
# chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name)
# chat_model.eval()
# 
# sentiment_analysis = pipeline("sentiment-analysis")
# 
# qa_model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
# qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)
# qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)
# 
# summarizer_model_name = "facebook/bart-large-cnn"
# summarizer_tokenizer = BartTokenizer.from_pretrained(summarizer_model_name)
# summarizer_model = BartForConditionalGeneration.from_pretrained(summarizer_model_name)
# summarizer_model.eval()
# 
# model_id = "CompVis/stable-diffusion-v1-4"
# pipe = StableDiffusionPipeline.from_pretrained(model_id)
# 
# # Evaluation metrics storage
# if 'feedback' not in st.session_state:
#     st.session_state['feedback'] = {"Task": [], "Rating": [], "Comments": []}
# 
# # Define Functions for Each Task
# def predict_next_word(prompt, model, tokenizer, top_k=5):
#     inputs = tokenizer(prompt, return_tensors='pt')
#     with torch.no_grad():
#         outputs = model(**inputs)
# 
#     # Extract logits for the next token and get top-k predictions
#     next_token_logits = outputs.logits[:, -1, :]
#     top_k_tokens = torch.topk(next_token_logits, top_k).indices[0].tolist()
#     predicted_tokens = [tokenizer.decode([token]) for token in top_k_tokens]
#     return predicted_tokens
# 
# def generate_text(prompt, model, tokenizer, max_length=50):
#     inputs = tokenizer(prompt, return_tensors='pt')
#     with torch.no_grad():
#         outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=1.0)
#     return tokenizer.decode(outputs[0], skip_special_tokens=True)
# 
# def chat_with_model(prompt, chat_history_ids, model, tokenizer):
#     new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')
#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids
#     chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)
#     response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
#     return response, chat_history_ids
# 
# def analyze_sentiment(text):
#     return sentiment_analysis(text)[0]
# 
# def answer_question(question, context):
#     inputs = qa_tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors="pt")
#     outputs = qa_model(**inputs)
#     answer_start = torch.argmax(outputs.start_logits)
#     answer_end = torch.argmax(outputs.end_logits) + 1
#     return qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs["input_ids"].tolist()[0][answer_start:answer_end]))
# 
# def summarize(text, model, tokenizer, max_length=130, min_length=30):
#     inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
#     summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)
#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)
# 
# def generate_image(prompt, pipe):
#     with torch.no_grad():
#         return pipe(prompt).images[0]
# 
# # Collect user satisfaction
# def collect_feedback(task_name, user_response):
#     st.session_state['feedback']["Task"].append(task_name)
#     st.session_state['feedback']["Rating"].append(st.select_slider("Rate the quality:", options=[1, 2, 3, 4, 5], key=task_name))
#     st.session_state['feedback']["Comments"].append(st.text_input("Any comments on this result?", key=f"{task_name}_comments"))
# 
# 
# # Streamlit app setup
# st.title("Multifunctional NLP & Image Generation Tool")
# st.write("Evaluate NLP and Image Generation tool performance using task-specific metrics.")
# 
# option = st.radio("Select a task:", ("Next Word Prediction", "Story Prediction", "Chatbot", "Sentiment Analysis", "Question Answering", "Summary Generation", "Image Generation"))
# 
# # Task interfaces
# if option == "Next Word Prediction":
#     user_input = st.text_input("Enter a prompt for next word prediction:", "")
#     if user_input:
#         predicted_words = predict_next_word(user_input, gpt2_model, gpt2_tokenizer)
#         st.write("Predicted next words:", predicted_words)
#         collect_feedback("Next Word Prediction", predicted_words)
# 
# elif option == "Story Prediction":
#     user_input = st.text_input("Enter a prompt to generate text:", "")
#     if user_input:
#         generated_text = generate_text(user_input, gpt2_model, gpt2_tokenizer)
#         st.write("Generated text:", generated_text)
#         collect_feedback("Text Generation", generated_text)
# 
# elif option == "Chatbot":
#     st.write("Start chatting with the bot below.")
#     user_input = st.text_input("You:", "")
#     if user_input:
#         response, _ = chat_with_model(user_input, None, chat_model, chat_tokenizer)
#         st.write("Bot:", response)
#         collect_feedback("Chatbot", response)
# 
# elif option == "Sentiment Analysis":
#     user_input = st.text_input("Enter a sentence for sentiment analysis:", "")
#     if user_input:
#         sentiment_result = analyze_sentiment(user_input)
#         st.write(f"Sentiment: {sentiment_result['label']} (Confidence: {sentiment_result['score']:.2f})")
#         collect_feedback("Sentiment Analysis", sentiment_result)
# 
# elif option == "Question Answering":
#     context = st.text_area("Enter the context:")
#     question = st.text_input("Enter your question:")
#     if context and question:
#         answer = answer_question(question, context)
#         st.write("Answer:", answer)
#         collect_feedback("Question Answering", answer)
# 
# elif option == "Summary Generation":
#     text = st.text_area("Enter text for summarization:")
#     if text:
#         summary = summarize(text, summarizer_model, summarizer_tokenizer)
#         st.write("Summary:", summary)
#         collect_feedback("Summary Generation", summary)
# 
# elif option == "Image Generation":
#     prompt = st.text_input("Enter a prompt for image generation:", "")
#     if prompt:
#         image = generate_image(prompt, pipe)
#         st.image(image, caption=f"Generated image for: {prompt}")
#         collect_feedback("Image Generation", image)
# 
# # Display feedback summary
# if st.button("Show Feedback Summary"):
#     st.write(st.session_state['feedback'])
#

!pip install pyngrok

"""# ngrok in this setup allows us to expose the local Streamlit app to the internet by creating a secure public URL.
#ngrok makes it easy to instantly share a local Streamlit app, making testing, feedback, and collaboration more efficient.

"""

# Import ngrok and start the tunnel on the default Streamlit port (8501)
from pyngrok import ngrok

# Set up authentication for ngrok
ngrok.set_auth_token("Your AuthToken")

# Start ngrok tunnel on http protocol with Streamlitâ€™s default port (8501)
public_url = ngrok.connect(8501, "http")
print("Streamlit is live at:", public_url)

# Run the Streamlit app in the background
!streamlit run NLP.py &